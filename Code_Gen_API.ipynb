{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb0kP6YCMhNT"
      },
      "source": [
        "## Install Required Packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN88z2SEMhNV",
        "outputId": "0a81c215-655e-4823-fa52-4d01b66f25dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.10.3)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Collecting slowapi\n",
            "  Downloading slowapi-0.1.9-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Collecting ollama\n",
            "  Downloading ollama-0.4.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.19-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.27.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx) (3.10)\n",
            "Collecting limits>=2.3 (from slowapi)\n",
            "  Downloading limits-3.14.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx) (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.10/dist-packages (from limits>=2.3->slowapi) (1.2.15)\n",
            "Requirement already satisfied: packaging<25,>=21 in /usr/local/lib/python3.10/dist-packages (from limits>=2.3->slowapi) (24.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx) (1.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2->limits>=2.3->slowapi) (1.17.0)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading slowapi-0.1.9-py3-none-any.whl (14 kB)\n",
            "Downloading ollama-0.4.4-py3-none-any.whl (13 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
            "Downloading python_multipart-0.0.19-py3-none-any.whl (24 kB)\n",
            "Downloading limits-3.14.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, python-multipart, pyngrok, starlette, limits, httpx, slowapi, ollama, fastapi\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.0\n",
            "    Uninstalling httpx-0.28.0:\n",
            "      Successfully uninstalled httpx-0.28.0\n",
            "Successfully installed fastapi-0.115.6 httpx-0.27.2 limits-3.14.1 ollama-0.4.4 pyngrok-7.2.1 python-multipart-0.0.19 slowapi-0.1.9 starlette-0.41.3 uvicorn-0.32.1\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn pydantic httpx slowapi torch ollama pyngrok python-multipart torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBbi7Fi1MhNW"
      },
      "source": [
        "## Import Dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n_T6mYoEMhNX"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import threading\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from typing import Optional, Dict, Any, Callable\n",
        "from dataclasses import dataclass\n",
        "from functools import wraps\n",
        "import atexit\n",
        "from slowapi import Limiter\n",
        "\n",
        "# Apply nest_asyncio for Jupyter\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G6RZLNmMhNY"
      },
      "source": [
        "## Logging Setup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-JswtGlWMhNY"
      },
      "outputs": [],
      "source": [
        "def setup_logging() -> Dict[str, str]:\n",
        "    \"\"\"Setup logging configuration and return log file paths\"\"\"\n",
        "    log_dir = \"ollama_logs\"\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    log_files = {\n",
        "        \"server\": os.path.join(log_dir, f\"ollama_server_{timestamp}.log\"),\n",
        "        \"install\": os.path.join(log_dir, f\"ollama_install_{timestamp}.log\"),\n",
        "        \"api\": os.path.join(log_dir, f\"api_{timestamp}.log\"),\n",
        "    }\n",
        "\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        "        handlers=[logging.FileHandler(log_files[\"api\"]), logging.StreamHandler()],\n",
        "    )\n",
        "\n",
        "    return log_files\n",
        "\n",
        "\n",
        "# Initialize logging\n",
        "log_files = setup_logging()\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPiHpbA9MhNZ"
      },
      "source": [
        "## Ollama Installation Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CbkgtdNrMhNZ"
      },
      "outputs": [],
      "source": [
        "def install_ollama(install_log_file: str) -> bool:\n",
        "    \"\"\"Install Ollama and return success status\"\"\"\n",
        "    logger.info(\"Starting Ollama installation...\")\n",
        "\n",
        "    def download_install_script() -> Optional[str]:\n",
        "        try:\n",
        "            process = subprocess.run(\n",
        "                [\"curl\", \"-L\", \"https://ollama.ai/install.sh\"],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                check=True,\n",
        "            )\n",
        "            return process.stdout\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            logger.error(f\"Failed to download install script: {e}\")\n",
        "            return None\n",
        "\n",
        "    def execute_install_script(script_content: str) -> bool:\n",
        "        script_path = \"install_ollama.sh\"\n",
        "        try:\n",
        "            with open(script_path, \"w\") as f:\n",
        "                f.write(script_content)\n",
        "            os.chmod(script_path, 0o755)\n",
        "\n",
        "            with open(install_log_file, \"w\") as log_file:\n",
        "                subprocess.run(\n",
        "                    [\"bash\", script_path],\n",
        "                    stdout=log_file,\n",
        "                    stderr=subprocess.STDOUT,\n",
        "                    check=True,\n",
        "                )\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Installation failed: {e}\")\n",
        "            return False\n",
        "        finally:\n",
        "            if os.path.exists(script_path):\n",
        "                os.remove(script_path)\n",
        "\n",
        "    script_content = download_install_script()\n",
        "    if script_content:\n",
        "        return execute_install_script(script_content)\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-whZoU1_MhNa"
      },
      "source": [
        "## Server Management Functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YqQLQ630MhNa"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ServerProcess:\n",
        "    process: Optional[subprocess.Popen] = None\n",
        "    log_file: Optional[str] = None\n",
        "\n",
        "\n",
        "def start_server(server_log_file: str) -> ServerProcess:\n",
        "    \"\"\"Start Ollama server and return process handle\"\"\"\n",
        "    logger.info(\"Starting Ollama server...\")\n",
        "    try:\n",
        "        process = subprocess.Popen(\n",
        "            [\"ollama\", \"serve\"],\n",
        "            stdout=open(server_log_file, \"w\"),\n",
        "            stderr=subprocess.STDOUT,\n",
        "            start_new_session=True,\n",
        "        )\n",
        "        time.sleep(5)  # Allow server initialization\n",
        "\n",
        "        if process.poll() is None:\n",
        "            logger.info(\"Ollama server started successfully\")\n",
        "            return ServerProcess(process=process, log_file=server_log_file)\n",
        "\n",
        "        logger.error(\"Server failed to start\")\n",
        "        return ServerProcess()\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error starting server: {e}\")\n",
        "        return ServerProcess()\n",
        "\n",
        "\n",
        "def stop_server(server: ServerProcess) -> None:\n",
        "    \"\"\"Stop Ollama server gracefully\"\"\"\n",
        "    if server.process:\n",
        "        try:\n",
        "            server.process.terminate()\n",
        "            server.process.wait(timeout=5)\n",
        "        except subprocess.TimeoutExpired:\n",
        "            server.process.kill()\n",
        "        logger.info(\"Server stopped\")\n",
        "\n",
        "\n",
        "def read_server_logs(server: ServerProcess) -> str:\n",
        "    \"\"\"Read server logs\"\"\"\n",
        "    if server.log_file and os.path.exists(server.log_file):\n",
        "        try:\n",
        "            with open(server.log_file, \"r\") as f:\n",
        "                return f.read()\n",
        "        except Exception as e:\n",
        "            return f\"Error reading logs: {e}\"\n",
        "    return \"Log file not available\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQl0JbhAMhNb"
      },
      "source": [
        "## Server Monitor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ie5QmND5MhNb"
      },
      "outputs": [],
      "source": [
        "def create_server_monitor(server: ServerProcess) -> threading.Thread:\n",
        "    \"\"\"Create a monitoring thread for the server\"\"\"\n",
        "\n",
        "    def monitor_function():\n",
        "        while True:\n",
        "            if server.process and server.process.poll() is None:\n",
        "                logger.info(\"Server status check:\")\n",
        "                logger.info(read_server_logs(server))\n",
        "            time.sleep(300)  # Check every 5 minutes\n",
        "\n",
        "    monitor_thread = threading.Thread(target=monitor_function, daemon=True)\n",
        "    monitor_thread.start()\n",
        "    return monitor_thread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsqkdAzwMhNb"
      },
      "source": [
        "## Server Initialization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nNcTYSA2MhNc"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, HTTPException, Depends, Request\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import JSONResponse\n",
        "from pydantic import BaseModel, Field\n",
        "from slowapi import Limiter, _rate_limit_exceeded_handler\n",
        "from slowapi.util import get_remote_address\n",
        "from slowapi.errors import RateLimitExceeded\n",
        "\n",
        "\n",
        "# API Models\n",
        "class CodeRequest(BaseModel):\n",
        "    prompt: str = Field(..., description=\"Code generation prompt\")\n",
        "    language: str = Field(default=\"python\", description=\"Target language\")\n",
        "    temperature: float = Field(default=0.7, ge=0, le=1)\n",
        "\n",
        "\n",
        "class ApiResponse(BaseModel):\n",
        "    success: bool\n",
        "    data: Dict[str, Any]\n",
        "    error: Optional[str] = None\n",
        "\n",
        "\n",
        "# Initialize FastAPI\n",
        "def create_app() -> FastAPI:\n",
        "    app = FastAPI(title=\"Ollama Code Generation API\")\n",
        "    limiter = Limiter(key_func=get_remote_address)\n",
        "\n",
        "    app.state.limiter = limiter\n",
        "    app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n",
        "\n",
        "    app.add_middleware(\n",
        "        CORSMiddleware,\n",
        "        allow_origins=[\"*\"],\n",
        "        allow_credentials=True,\n",
        "        allow_methods=[\"*\"],\n",
        "        allow_headers=[\"*\"],\n",
        "    )\n",
        "\n",
        "    return app\n",
        "\n",
        "\n",
        "app = create_app()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM46UnE5MhNc"
      },
      "source": [
        "## FastAPI Setup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "37ecLfvoMhNc"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, HTTPException, Depends, Request\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import JSONResponse\n",
        "from pydantic import BaseModel, Field\n",
        "from ollama import AsyncClient\n",
        "from slowapi import Limiter, _rate_limit_exceeded_handler\n",
        "from slowapi.util import get_remote_address\n",
        "from slowapi.errors import RateLimitExceeded\n",
        "\n",
        "\n",
        "# API Models\n",
        "class CodeRequest(BaseModel):\n",
        "    prompt: str = Field(..., description=\"Code generation prompt\")\n",
        "    language: str = Field(default=\"python\", description=\"Target language\")\n",
        "    temperature: float = Field(default=0.7, ge=0, le=1)\n",
        "\n",
        "\n",
        "class ApiResponse(BaseModel):\n",
        "    success: bool\n",
        "    data: Dict[str, Any]\n",
        "    error: Optional[str] = None\n",
        "\n",
        "\n",
        "# Initialize FastAPI\n",
        "def create_app() -> FastAPI:\n",
        "    app = FastAPI(title=\"Ollama Code Generation API\")\n",
        "    limiter = Limiter(key_func=get_remote_address) # Define limiter here\n",
        "\n",
        "    app.state.limiter = limiter\n",
        "    app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n",
        "\n",
        "    app.add_middleware(\n",
        "        CORSMiddleware,\n",
        "        allow_origins=[\"*\"],\n",
        "        allow_credentials=True,\n",
        "        allow_methods=[\"*\"],\n",
        "        allow_headers=[\"*\"],\n",
        "    )\n",
        "\n",
        "    return app, limiter # Return limiter from create_app()\n",
        "\n",
        "\n",
        "app, limiter = create_app() # Unpack app and limiter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2OQ-Re6MhNc"
      },
      "source": [
        "## API Endpoints:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Mkc7wvrvMhNd"
      },
      "outputs": [],
      "source": [
        "from ollama import AsyncClient\n",
        "\n",
        "\n",
        "from ollama import AsyncClient\n",
        "\n",
        "@app.post(\"/generate\", response_model=ApiResponse)\n",
        "@limiter.limit(\"10/minute\")\n",
        "async def generate_code(request: CodeRequest, request_obj: Request):  # Fixed parameter name\n",
        "    try:\n",
        "        response = await AsyncClient().chat(\n",
        "            model=\"wizardcoder\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Generate {request.language} code: {request.prompt}\",\n",
        "                }\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        return ApiResponse(\n",
        "            success=True,\n",
        "            data={\"generated_code\": response.message.content}\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Generation error: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"gpu_available\": torch.cuda.is_available(),\n",
        "        \"server_logs\": read_server_logs(ollama_server),\n",
        "    }\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\n",
        "        \"message\": \"RAG QA API\",\n",
        "        \"version\": \"1.0.0\",\n",
        "        \"documentation\": \"/docs\",\n",
        "        \"health\": \"/health\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing the wizardcoder"
      ],
      "metadata": {
        "id": "bK8SmAcvPUsY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRbvpF9VMhNd"
      },
      "source": [
        "## Server Start with Ngrok:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw-rT_WAMhNd",
        "outputId": "3c04e976-7fa3-4bb6-989f-a48052fc058b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Failed to start Ollama service via systemctl, trying alternative method\n",
            "INFO:     Started server process [921]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 8000): address already in use\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API is accessible at: NgrokTunnel: \"https://a320-35-245-244-178.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import subprocess\n",
        "import shutil\n",
        "import os\n",
        "import requests\n",
        "\n",
        "def install_ollama():\n",
        "    \"\"\"Install Ollama using the official install script\"\"\"\n",
        "    try:\n",
        "        # Download the install script\n",
        "        install_script = requests.get(\"https://ollama.ai/install.sh\").text\n",
        "\n",
        "        # Save the script\n",
        "        with open(\"install_ollama.sh\", \"w\") as f:\n",
        "            f.write(install_script)\n",
        "\n",
        "        # Make the script executable\n",
        "        os.chmod(\"install_ollama.sh\", 0o755)\n",
        "\n",
        "        # Run the install script\n",
        "        subprocess.run([\"sudo\", \"./install_ollama.sh\"], check=True)\n",
        "\n",
        "        # Clean up\n",
        "        os.remove(\"install_ollama.sh\")\n",
        "\n",
        "        # Verify installation\n",
        "        subprocess.run([\"ollama\", \"--version\"], check=True)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to install Ollama: {e}\")\n",
        "        return False\n",
        "\n",
        "def start_api_server(port: int = 8000):\n",
        "    \"\"\"Start FastAPI server with ngrok tunnel\"\"\"\n",
        "\n",
        "    # Check if ollama is installed and install if needed\n",
        "    if not shutil.which('ollama'):\n",
        "        logger.info(\"Ollama not found. Installing...\")\n",
        "        if not install_ollama():\n",
        "            raise RuntimeError(\"Failed to install Ollama\")\n",
        "        logger.info(\"Ollama installed successfully\")\n",
        "\n",
        "    # Start Ollama service\n",
        "    try:\n",
        "        subprocess.run([\"sudo\", \"systemctl\", \"start\", \"ollama\"], check=True)\n",
        "    except subprocess.CalledProcessError:\n",
        "        logger.warning(\"Failed to start Ollama service via systemctl, trying alternative method\")\n",
        "        try:\n",
        "            subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to start Ollama service: {e}\")\n",
        "\n",
        "    def run_server():\n",
        "        uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
        "\n",
        "    # Start server in thread\n",
        "    server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "    server_thread.start()\n",
        "\n",
        "    # Setup ngrok\n",
        "    ngrok.set_auth_token(\"2DqArVFoOn6ptBy4F8by2rV7eVl_HnmvlCipjgjzuxMiRCwb\")  # Your token\n",
        "    public_url = ngrok.connect(port)\n",
        "    logger.info(f\"Public URL: {public_url}\")\n",
        "\n",
        "    # Verify ollama service is running\n",
        "    try:\n",
        "        subprocess.run([\"ollama\", \"list\"], check=True)\n",
        "    except subprocess.CalledProcessError:\n",
        "        logger.error(\"Ollama service is not running properly\")\n",
        "        raise RuntimeError(\"Ollama service check failed\")\n",
        "\n",
        "    return public_url, server_thread\n",
        "\n",
        "try:\n",
        "    # Start API server\n",
        "    public_url, api_thread = start_api_server()\n",
        "    print(f\"API is accessible at: {public_url}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to start API server: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkPz-ecXMhNe"
      },
      "source": [
        "## Test Request:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FjYg28zMhNe",
        "outputId": "b3535cfb-e457-4a60-facb-96eb8aa5a16f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-4' coro=<Server.serve() done, defined at /usr/local/lib/python3.10/dist-packages/uvicorn/server.py:67> exception=SystemExit(1)>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 162, in startup\n",
            "    server = await loop.create_server(\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1519, in create_server\n",
            "    raise OSError(err.errno, 'error while attempting '\n",
            "OSError: [Errno 98] error while attempting to bind on address ('0.0.0.0', 8000): address already in use\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-13-64d973a3f26e>\", line 55, in run_server\n",
            "    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/main.py\", line 579, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 65, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 315, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    await self._serve(sockets)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 84, in _serve\n",
            "    await self.startup(sockets=sockets)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 172, in startup\n",
            "    sys.exit(1)\n",
            "SystemExit: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     35.245.244.178:0 - \"POST /generate HTTP/1.1\" 500 Internal Server Error\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 715, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 735, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 288, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 76, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 73, in app\n",
            "    response = await f(request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 301, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/slowapi/extension.py\", line 725, in async_wrapper\n",
            "    raise Exception(\n",
            "Exception: parameter `request` must be an instance of starlette.requests.Request\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error making request: 500 Server Error: Internal Server Error for url: https://a320-35-245-244-178.ngrok-free.app/generate\n",
            "Response text: Internal Server Error\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def test_api(url: str):\n",
        "    test_request = {\n",
        "        \"prompt\": \"Create a function that calculates fibonacci numbers\",\n",
        "        \"language\": \"python\",\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(f\"{url}/generate\", json=test_request)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "        print(json.dumps(response.json(), indent=2))\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error making request: {e}\")\n",
        "        if hasattr(e.response, 'text'):\n",
        "            print(f\"Response text: {e.response.text}\")\n",
        "\n",
        "# Run test\n",
        "test_api(\"https://a320-35-245-244-178.ngrok-free.app\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_87S6JdMhNe"
      },
      "source": [
        "## view logs during operation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMXFmGaCMhNf"
      },
      "outputs": [],
      "source": [
        "# View current server logs\n",
        "print(read_server_logs(ollama_server))\n",
        "\n",
        "# View all logs\n",
        "for log_type, log_file in log_files.items():\n",
        "    print(f\"\\n=== {log_type.upper()} LOGS ===\")\n",
        "    with open(log_file, \"r\") as f:\n",
        "        print(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the server\n",
        "!kill -9 $(lsof -t -i:8000)\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "5CK8EBiv657X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}